import uvicorn
from fastapi import FastAPI, UploadFile, File, Form
from transformers import MT5ForConditionalGeneration, MT5Tokenizer, pipeline
import PyPDF2

# ------------------- Model Setup -------------------
MODEL_NAME = "google/mt5-small"

tokenizer = MT5Tokenizer.from_pretrained(MODEL_NAME)
model = MT5ForConditionalGeneration.from_pretrained(MODEL_NAME)
generator = pipeline("text2text-generation", model=model, tokenizer=tokenizer)

app = FastAPI(
    title="Paper AI Assistant",
    description="ğŸ“‘ ØªØµÙ†ÙŠÙ ÙˆØªÙ„Ø®ÙŠØµ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚ Ø§Ù„Ø¹Ù„Ù…ÙŠØ© + Ø£Ø³Ø¦Ù„Ø© ÙˆØ£Ø¬ÙˆØ¨Ø©"
)

# ------------------- Utils -------------------
def extract_text_from_pdf(file) -> str:
    """Extract text from uploaded PDF file"""
    pdf_reader = PyPDF2.PdfReader(file)
    text = ""
    for page in pdf_reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text + "\n"
    return text.strip()

def ask_model(prompt: str, max_new_tokens=500):
    """Generate text using mT5 model"""
    outputs = generator(
        prompt,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.7
    )
    return outputs[0]["generated_text"]

# ------------------- Endpoints -------------------
@app.post("/upload-paper/")
async def upload_paper(file: UploadFile = File(...)):
    """Upload PDF, classify & summarize it"""
    text = extract_text_from_pdf(file.file)

    classify_prompt = f"ØµÙ†Ù‘Ù Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ±Ù‚Ø© Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø­Ø³Ø¨ Ù…Ø¬Ø§Ù„Ù‡Ø§ ÙˆÙ†ÙˆØ¹Ù‡Ø§:\n{text[:1500]}"
    classification = ask_model(classify_prompt, max_new_tokens=200)

    summary_prompt = f"Ù„Ø®Ù‘Øµ Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ±Ù‚Ø© Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ Ù†Ù‚Ø§Ø· ÙˆØ§Ø¶Ø­Ø©:\n{text[:4000]}"
    summary = ask_model(summary_prompt, max_new_tokens=400)

    return {
        "classification": classification,
        "summary": summary,
        "full_text": text[:5000]
    }

@app.post("/ask/")
async def ask_question(
    question: str = Form(...),
    context: str = Form(...)
):
    """Ask a question about the paper's context"""
    qa_prompt = f"Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ù† ÙˆØ±Ù‚Ø© Ø¹Ù„Ù…ÙŠØ©:\n{context}\n\nØ§Ù„Ø³Ø¤Ø§Ù„: {question}\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"
    answer = ask_model(qa_prompt, max_new_tokens=300)
    return {"answer": answer}

# ------------------- Run -------------------
if __name__ == "__main__":
    uvicorn.run("paper_ai:app", host="0.0.0.0", port=8000, reload=True)
